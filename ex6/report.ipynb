{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontani0930/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME='bert-base-uncased'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパスのロード\n",
    "ma_reuters = LazyCorpusLoader(\n",
    "    'ma_reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n",
    "    cat_file='cats.txt', encoding='ISO-8859-2')\n",
    "\n",
    "# MA_Reutersのロード\n",
    "documents = ma_reuters.fileids()\n",
    "\n",
    "# 訓練とテストデータの文書IDの抽出\n",
    "train_docs_id = [doc for doc in documents if doc.startswith(\"train\")]\n",
    "test_docs_id = [doc for doc in documents if doc.startswith(\"test\")]\n",
    "\n",
    "# 訓練とテストデータの生データの抽出\n",
    "train_docs = [ma_reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [ma_reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "\n",
    "# カテゴリーのリスト\n",
    "categories = ma_reuters.categories()\n",
    "num_categories = len(categories)\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id)\n",
    "                                  for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id)\n",
    "                             for doc_id in test_docs_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # BertModelのロード\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        # 線形変換を初期化しておく\n",
    "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        # データを入力しBERTの最終層の出力を得る\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)  # attention_mask:paddingじゃないトークンのこと\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "\n",
    "        # [PAD]以外のトークンで隠れ状態の平均を取る\n",
    "        averaged_hidden_state = (\n",
    "            last_hidden_state*attention_mask.unsqueeze(-1)).sum(1)/attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        # 線形変換\n",
    "        scores = self.linear(averaged_hidden_state)\n",
    "\n",
    "        # 出力の形式を整える\n",
    "        output = {\"logits\": scores}\n",
    "\n",
    "        # labelsが入力に含まれていたら、損失を計算し出力する\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        # 属性でアクセスできるようにする\n",
    "        output = type(\"bert_output\", (object,), output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザのロード\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for i in range(len(train_docs)):\n",
    "    text=train_docs[i]\n",
    "    labels=train_labels[i]\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    encoding['labels'] = labels\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) \n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.9*n)\n",
    "n_val = int(0.1*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "\n",
    "#　データセットからデータローダを作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontani0930/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:445: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "            model_name, num_labels=num_labels\n",
    "        ) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_scml(**batch)\n",
    "        scores = output.logits\n",
    "        labels_predicted = ( scores > 0 ).int()\n",
    "        num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "        accuracy = num_correct/scores.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=10,\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "model = BertForSequenceClassificationMultiLabel_pl(\n",
    "    MODEL_NAME, \n",
    "    num_labels=55, \n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontani0930/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /nas_home/kontani0930/myroom/workspace/data_science/ex6/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "\n",
      "  | Name      | Type                                    | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | bert_scml | BertForSequenceClassificationMultiLabel | 109 M \n",
      "----------------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "438.098   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontani0930/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontani0930/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 221/221 [01:00<00:00,  3.67it/s, loss=0.0213, v_num=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 221/221 [01:05<00:00,  3.35it/s, loss=0.0213, v_num=11]\n"
     ]
    }
   ],
   "source": [
    "#学習\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "#test = trainer.test(dataloaders=dataloader_test)\n",
    "#print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=BertForSequenceClassificationMultiLabel_pl.load_from_checkpoint(\"model/epoch=4-step=1025.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPUのメモリ容量の都合でテストデータを分割\n",
    "test_docs_split=[test_docs[0:900],test_docs[900:1800],test_docs[1800:]]\n",
    "labels_predicted=[]\n",
    "bert_scml = model.bert_scml.cuda()\n",
    "\n",
    "for i in range(3): #推論\n",
    "    encoding=tokenizer(test_docs_split[i],max_length=128,padding='max_length',truncation=True,return_tensors=\"pt\")\n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "    with torch.no_grad():\n",
    "        output=bert_scml(**encoding)\n",
    "    scores=output.logits\n",
    "    label=(scores>0).int().cpu().numpy().tolist()\n",
    "    labels_predicted+=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard係数による評価:0.849\n",
      "Hamming損失による評価:0.006\n"
     ]
    }
   ],
   "source": [
    "# Jaccard係数の計算\n",
    "print(f\"Jaccard係数による評価:\"\n",
    "      f\"{np.round(jaccard_score(test_labels,labels_predicted, average='samples'),3)}\")\n",
    "# Hamming損失の計算\n",
    "print(f\"Hamming損失による評価:\"\n",
    "      f\"{np.round(hamming_loss(test_labels,labels_predicted),3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89f142c5e0f1705deb2db4cf7e0dc678636cb1933d453fbbccc55a26e1a8c8e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
