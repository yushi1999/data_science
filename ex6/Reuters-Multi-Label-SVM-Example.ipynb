{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このJupyter notebookを実行するにあたり、まず、\n",
    "    scikit-learn\n",
    "    nltk\n",
    "の２つのパッケージを入手し、インストールしておいてください。\n",
    "\n",
    "Pythonは3.6以上(fフォーマットのプリントが使える環境）を仮定しています。\n",
    "\n",
    "\n",
    "そのうえで、以下の(NLTKに関する)作業を一度だけ行ってください。\n",
    "\n",
    "\n",
    "python\n",
    "バージョン情報などが出力されます。\n",
    "\n",
    "\n",
    "\n",
    "例\n",
    "(py310) G:>python\n",
    "Python 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)] on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ここでGUIでコーパスなどのダウンロードを促す画面が出る場合は指示に従い全部インストールしてください。その際、Download Directoryを必ずメモしておいてください。\n",
    "\n",
    "\n",
    "CUIの場合、以下のようにしてコーパス等をインストールしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTKを使ったコーパスのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10700  記事総数\n",
      "7713 訓練データ\n",
      "2987 テストデータ\n",
      "55 カテゴリー\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'cocoa', 'coffee', 'copper', 'corn', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'hog', 'housing', 'interest', 'ipi', 'iron-steel', 'jobs', 'lead', 'livestock', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'oilseed', 'orange', 'palm-oil', 'pet-chem', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import *\n",
    "\n",
    "# コーパスのロード\n",
    "ma_reuters = LazyCorpusLoader(\n",
    "    'ma_reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n",
    "    cat_file='cats.txt', encoding='ISO-8859-2')\n",
    "\n",
    "# MA_Reutersのロード\n",
    "documents = ma_reuters.fileids()\n",
    "print( f\"{str(len(documents))}  記事総数\")\n",
    "\n",
    "# 訓練とテストデータの文書IDの抽出\n",
    "train_docs_id = [doc for doc in documents if doc.startswith(\"train\")]\n",
    "test_docs_id = [doc for doc in documents if doc.startswith(\"test\")]\n",
    "\n",
    "print( f\"{str(len(train_docs_id))} 訓練データ\")\n",
    "print( f\"{str(len(test_docs_id))} テストデータ\")\n",
    "\n",
    "# 訓練とテストデータの生データの抽出\n",
    "train_docs = [ma_reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [ma_reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "\n",
    "# カテゴリーのリスト\n",
    "categories = ma_reuters.categories()\n",
    "num_categories = len(categories)\n",
    "\n",
    "print( f\"{num_categories} カテゴリー\")\n",
    "print( f\"{categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カテゴリーにある記事を表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAPAN MINISTRY SAYS OPEN FARM TRADE WOULD HIT U.S.\n",
      "  Japan's Agriculture Ministry, angered by\n",
      "  U.S. Demands that Japan open its farm products market, will\n",
      "  tell U.S. Officials at talks later this month that\n",
      "  liberalisation would harm existing U.S. Farm exports to Japan,\n",
      "  a senior ministry official said.\n",
      "      \"Imports from the U.S. Would drop due to active sales drives\n",
      "  by other suppliers,\" the official, who declined to be named,\n",
      "  said. \"Japan is the largest customer for U.S. Farm products and\n",
      "  it is not reasonable for the U.S. To demand Japan liberalise\n",
      "  its farm import market,\" he said.\n",
      "      Agriculture Minister Mutsuki Kato has said if the U.S.\n",
      "  Insists Japan open its protected rice market it will also open\n",
      "  its wheat market, where volume and origin are regulated to\n",
      "  protect local farmers.\n",
      "      Australia and Canada could then increase their wheat\n",
      "  exports as they are more competitive than the U.S., He said.\n",
      "  End-users would also buy other origins, grain traders said.\n",
      "      U.S. Agriculture Secretary Richard Lyng, who is due to\n",
      "  visit Japan for talks between April 16-27, has said he will ask\n",
      "  Japan to offer a share of its rice market to U.S. Suppliers and\n",
      "  remove quotas on U.S. Beef and citrus imports.\n",
      "      Other countries are already cutting into the U.S. Market\n",
      "  share here. Australia, the largest beef supplier to Japan, has\n",
      "  been trying to boost exports prior to the expiry of a four-year\n",
      "  beef accord next March 31.\n",
      "      Imports of U.S. Corn have fallen due to increased sales\n",
      "  from China and South America, while Japanese soybean imports\n",
      "  from Brazil are expected to rise sharply this year, although\n",
      "  the U.S. Will remain the largest supplier.\n",
      "      U.S. Feedgrain sales will also drop if Japan opens up its\n",
      "  beef imports, since Japan depends almost entirely on feedgrain\n",
      "  imports, mainly from the U.S., Japanese officials said.\n",
      "      An indication of the U.S. Position came last December when\n",
      "  Under Secretary of Agriculture Daniel Amstutz said Japan has\n",
      "  the potential to provide one of the largest boosts to U.S.\n",
      "  Agricultural exports, with the beef market alone representing\n",
      "  some one billion dlrs in new business.\n",
      "      The U.S. Has also asked the General Agreement on Tariffs\n",
      "  and Trade to investigate the legality of Japanese import\n",
      "  controls on 12 other farm products, including fruit juices,\n",
      "  purees and pulp, tomato juice, ketchup and sauce, peanuts,\n",
      "  prepared beef products and miscellaneous beans.\n",
      "      To help calm heated trade relations with the U.S., Japan's\n",
      "  top business group Keidanren has urged the government to remove\n",
      "  residual import restrictions on agricultural products.\n",
      "      But Agriculture Minister Kato has ruled out any emotional\n",
      "  reaction, and the senior ministry official said the farm issue\n",
      "  should not become a scapegoat for trade pressure in the\n",
      "  industrial sector.\n",
      "      \"Japan is the largest buyer of U.S. Farm products, and these\n",
      "  issues should not be discussed on the same table,\" the official\n",
      "  said.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 生の文書例\n",
    "# Documents in a category\n",
    "category_docs = ma_reuters.fileids(\"soybean\");\n",
    "document_id = category_docs[0] # オレンジ・カテゴリーの最初の文書\n",
    "\n",
    "# 記事の中身を表示\n",
    "print( f\"{ma_reuters.raw(document_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTKを用いたテキストのトークン抽出, scikit-learnを用いたテキストのTF-IDFモデル抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IFモデルに変換しました\n",
      "訓練データの文書数x次元数：(7713, 26978)\n",
      "テストデータの文書数x次元数：(2987, 26978)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import re # 正規表現\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "def tokenize(text): # テキストを小文字英語トークンに変換してリストで返す関数\n",
    "    min_length = 3 # 3文字以上のものだけ残す\n",
    "    words = [word.lower() for word in word_tokenize(text)]\n",
    "    p = re.compile('[a-zA-Z]+') # 小文字化しているが一応アルファベットで開始\n",
    "    filtered_tokens = [token for token in words \\\n",
    "                       if p.match(token) and len(token) >= min_length]\n",
    "    return filtered_tokens\n",
    "\n",
    "# TF-IDF重みでベクトル化\n",
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "\n",
    "# 訓練データはfit_transform関数で決められた語彙に基づきTF-IDFを計算\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "\n",
    "# テストデータはtransform関数で決められた語彙に基づきTF-IDFを計算\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "\n",
    "print(f\"TF-IFモデルに変換しました\")\n",
    "print(f\"訓練データの文書数x次元数：{vectorised_train_documents.shape}\")\n",
    "print(f\"テストデータの文書数x次元数：{vectorised_test_documents.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMによる分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(OVR_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard係数による評価:0.86\n",
      "Hamming損失による評価:0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id)\n",
    "                                  for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id)\n",
    "                             for doc_id in test_docs_id])\n",
    "\n",
    "# マルチクラス　マルチラベル分類器で訓練＋予測\n",
    "OVR_classifier = OneVsRestClassifier(LinearSVC(random_state=41))\n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "# Jaccard係数の計算\n",
    "print(f\"Jaccard係数による評価:\"\n",
    "      f\"{np.round(jaccard_score(test_labels,OVR_predictions, average='samples'),3)}\")\n",
    "\n",
    "# Hamming損失の計算\n",
    "print(f\"Hamming損失による評価:\"\n",
    "      f\"{np.round(hamming_loss(test_labels,OVR_predictions),3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_labels_split=np.empty([55,100,55])\\npredictions_split=np.empty([55,100,55])\\nfor i in range(clen):\\n    new_index=np.argmax(test_labels[0])\\n    np.append(test_labels_split[new_index],test_labels[i],axis=0)\\n    np.append(predictions_split[new_index],OVR_predictions[new_index],axis=0)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clen=2987\n",
    "test_labels_split=[]\n",
    "predictions_split=[]\n",
    "\n",
    "for i in range(clen):\n",
    "    test_labels_split.append(np.argmax(test_labels[i]))\n",
    "    predictions_split.append(np.argmax(OVR_predictions[i]))\n",
    "\n",
    "\"\"\"test_labels_split=np.empty([55,100,55])\n",
    "predictions_split=np.empty([55,100,55])\n",
    "for i in range(clen):\n",
    "    new_index=np.argmax(test_labels[0])\n",
    "    np.append(test_labels_split[new_index],test_labels[i],axis=0)\n",
    "    np.append(predictions_split[new_index],OVR_predictions[new_index],axis=0)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89f142c5e0f1705deb2db4cf7e0dc678636cb1933d453fbbccc55a26e1a8c8e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
